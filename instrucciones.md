# Instrucciones de Uso del Proyecto ETL con Airflow y Amazon Redshift ğŸš€

âœ… 1. *Clonar el Repositorio:* Primero, clona este repositorio en tu mÃ¡quina local.

âœ… 2. *Verificar Dependencias:* AsegÃºrate de tener Docker y Python instalados en tu equipo.

âœ… 3. *Ejecutar Docker Compose:* Ejecuta el siguiente comando para levantar los servicios necesarios:

   ```bash
   docker-compose up -d

Esto iniciarÃ¡ los contenedores necesarios para la aplicaciÃ³n.

âœ… 4. *Accede a la UI de Airflow:* Abre tu navegador web y navega a la interfaz de usuario de Airflow. La URL por defecto es http://localhost:8080.

âœ… 5. *Inicia SesiÃ³n en Airflow:* Utiliza las siguientes credenciales para iniciar sesiÃ³n:

Usuario: admin
ContraseÃ±a: (Sigue las instrucciones a continuaciÃ³n para obtenerla).
Obtener la ContraseÃ±a de Airflow: Utiliza el siguiente comando para obtener la contraseÃ±a de Airflow desde el contenedor del servicio web:
 

   ```bash
   docker exec -it [ID_DEL_CONTENEDOR_DEL_WEBSERVICE] cat standalone_admin_password.txt

ğŸ“Reemplaza [ID_DEL_CONTENEDOR_DEL_WEBSERVICE] con el ID del contenedor del servicio web.

âœ… 6. *Configurar Variables en Airflow:* Una vez iniciada la sesiÃ³n en la UI de Airflow, ve a la secciÃ³n "Admin" y selecciona "Variables". Carga el archivo con las credenciales necesarias para ejecutar el DAG.

Â¡Listo para Utilizar! Ahora, la aplicaciÃ³n estÃ¡ lista para su uso. Puedes ejecutar el DAG para obtener los datos meteorolÃ³gicos de las principales ciudades de SudamÃ©rica y realizar anÃ¡lisis u otras operaciones con ellos.

Â¡Disfruta explorando y utilizando esta aplicaciÃ³n para automatizar tus procesos de ETL y obtener datos actualizados del clima! Si tienes alguna pregunta o necesitas asistencia adicional, no dudes en contactarnos.

Â¡Feliz anÃ¡lisis de datos! ğŸŒ¦ï¸ğŸ“Š