# Instrucciones de Uso del Proyecto ETL con Airflow y Amazon Redshift 

1. *Clonar el Repositorio:* Primero, clona este repositorio en tu m谩quina local.

2. *Verificar Dependencias:* Aseg煤rate de tener Docker y Python instalados en tu equipo.

3. *Ejecutar Docker Compose:* Ejecuta el siguiente comando para levantar los servicios necesarios:

   ```bash
   docker-compose up -d

Esto iniciar谩 los contenedores necesarios para la aplicaci贸n.

4. *Accede a la UI de Airflow:* Abre tu navegador web y navega a la interfaz de usuario de Airflow. La URL por defecto es http://localhost:8080.

5. *Inicia Sesi贸n en Airflow:* Utiliza las siguientes credenciales para iniciar sesi贸n:

Usuario: admin
Contrase帽a: (Sigue las instrucciones a continuaci贸n para obtenerla).
Obtener la Contrase帽a de Airflow: Utiliza el siguiente comando para obtener la contrase帽a de Airflow desde el contenedor del servicio web:
docker exec -it [ID_DEL_CONTENEDOR_DEL_WEBSERVICE] cat standalone_admin_password.txt
Reemplaza [ID_DEL_CONTENEDOR_DEL_WEBSERVICE] con el ID del contenedor del servicio web.

6. *Configurar Variables en Airflow:* Una vez iniciada la sesi贸n en la UI de Airflow, ve a la secci贸n "Admin" y selecciona "Variables". Carga el archivo con las credenciales necesarias para ejecutar el DAG.

隆Listo para Utilizar! Ahora, la aplicaci贸n est谩 lista para su uso. Puedes ejecutar el DAG para obtener los datos meteorol贸gicos de las principales ciudades de Sudam茅rica y realizar an谩lisis u otras operaciones con ellos.

隆Disfruta explorando y utilizando esta aplicaci贸n para automatizar tus procesos de ETL y obtener datos actualizados del clima! Si tienes alguna pregunta o necesitas asistencia adicional, no dudes en contactarnos.

隆Feliz an谩lisis de datos! 锔